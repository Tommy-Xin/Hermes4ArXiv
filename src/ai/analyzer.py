#!/usr/bin/env python3
"""
AI åˆ†æå™¨æ¨¡å—
æ”¯æŒå¤šç§AIæ¨¡å‹ï¼šæ™ºè°±GLMã€DeepSeekç­‰
"""

import logging
import time
import json
from typing import Dict, Any, List

import openai
from tenacity import retry, stop_after_attempt, wait_exponential

from ..config import Config
from .prompts import PromptManager

logger = logging.getLogger(__name__)


class DeepSeekAnalyzer:
    """
    AIè®ºæ–‡åˆ†æå™¨ï¼Œæ”¯æŒå¤šç§AIæ¨¡å‹ã€‚
    æ”¯æŒå®Œæ•´çš„ä¸¤é˜¶æ®µåˆ†ææµç¨‹ã€‚
    """

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–åˆ†æå™¨ï¼Œä»é…ç½®ä¸­åŠ è½½è®¾ç½®ã€‚
        """
        self.config = config
        self.timeout = config.API_TIMEOUT

        # è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨å“ªä¸ªAPI
        if config.QWEN_API_KEY:
            # ä¼˜å…ˆä½¿ç”¨Qwen
            logger.info("ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œåˆ†æ")
            self.model = config.QWEN_MODEL or "qwen3-max"
            self.provider = "qwen"
            self.client = openai.OpenAI(
                api_key=config.QWEN_API_KEY,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        elif config.GLM_API_KEY:
            # ä½¿ç”¨æ™ºè°±GLMï¼ˆæ¬¡ä¼˜é€‰æ‹©ï¼‰
            from zhipuai import ZhipuAI
            logger.info("ä½¿ç”¨æ™ºè°±GLMæ¨¡å‹è¿›è¡Œåˆ†æ")
            self.model = config.GLM_MODEL or "glm-4.6"
            self.provider = "glm"
            self.client = ZhipuAI(api_key=config.GLM_API_KEY)
        elif config.DEEPSEEK_API_KEY:
            # ä½¿ç”¨DeepSeek
            logger.info("ä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡Œåˆ†æ")
            self.model = config.DEEPSEEK_MODEL or "deepseek-chat"
            self.provider = "deepseek"
            self.client = openai.OpenAI(
                api_key=config.DEEPSEEK_API_KEY,
                base_url="https://api.deepseek.com/v1"
            )
        else:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„APIå¯†é’¥ã€‚è¯·é…ç½® QWEN_API_KEYã€GLM_API_KEY æˆ– DEEPSEEK_API_KEY")

    def _create_completion(self, messages: List[Dict[str, str]], max_tokens: int, temperature: float, **kwargs) -> str:
        """
        ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£ï¼Œå¤„ç†ä¸åŒproviderçš„å·®å¼‚
        """
        try:
            if self.provider == "glm":
                # æ™ºè°±GLMä¸æ”¯æŒresponse_formatå’Œtimeoutå‚æ•°
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
            else:
                # Qwenå’ŒDeepSeekéƒ½æ”¯æŒå®Œæ•´çš„OpenAIå‚æ•°
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    **kwargs
                )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def rank_papers_in_batch(self, papers: list[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹ä¸€å°æ‰¹è®ºæ–‡è¿›è¡Œå¼ºåˆ¶æ’åå’Œè¯„åˆ† (Stage 1).
        è¿”å›ä¸€ä¸ªåŒ…å«è¯„åˆ†ç»“æœçš„åˆ—è¡¨ã€‚
        """
        logger.info(f"Executing Stage 1: Ranking a batch of {len(papers)} papers using {self.provider}.")
        if not papers:
            return []

        response_text = ""
        try:
            system_prompt = PromptManager.get_stage1_ranking_system_prompt()
            user_prompt = PromptManager.format_stage1_ranking_prompt(papers)

            # æ ¹æ®provideré€‰æ‹©åˆé€‚çš„å‚æ•°
            if self.provider == "glm":
                response_text = self._create_completion(
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                    max_tokens=2048,
                    temperature=0.2
                )
            else:
                response_text = self._create_completion(
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                    max_tokens=2048,
                    temperature=0.2,
                    response_format={"type": "json_object"},
                    timeout=self.timeout
                )
            logger.debug(f"Raw Stage 1 ranking response from AI: {response_text}")
            
            parsed_json = json.loads(response_text)
            
            if isinstance(parsed_json, dict):
                ranking_list = next((v for v in parsed_json.values() if isinstance(v, list)), None)
                if ranking_list is None:
                    logger.error("AI returned a JSON object for ranking, but no list was found inside.")
                    return []
            elif isinstance(parsed_json, list):
                ranking_list = parsed_json
            else:
                logger.error(f"AI ranking response was not a JSON list or a dict containing a list. Type: {type(parsed_json)}")
                return []

            if not all('paper_id' in item and 'score' in item for item in ranking_list):
                logger.error("AI ranking response list has malformed items.")
                return []
                
            return ranking_list

        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON from AI ranking response: {e}\nProblematic text: {response_text}")
            return []
        except Exception as e:
            logger.error(f"An unexpected error occurred during paper ranking: {e}", exc_info=True)
            return []

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def analyze_papers_batch(self, papers: list[Dict[str, Any]]) -> str:
        """
        å¯¹ä¸€æ‰¹è®ºæ–‡è¿›è¡Œæ·±å…¥çš„æ‰¹é‡åˆ†æ (Stage 2).
        è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰åˆ†æçš„é•¿å­—ç¬¦ä¸²ã€‚
        """
        logger.info(f"Executing Stage 2: Performing deep analysis on a batch of {len(papers)} papers using {self.provider}.")
        if not papers:
            return ""

        system_prompt = PromptManager.get_system_prompt()
        user_prompt = PromptManager.format_batch_analysis_prompt(papers)

        if self.provider == "glm":
            analysis_text = self._create_completion(
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                max_tokens=8000,
                temperature=0.5
            )
        else:
            analysis_text = self._create_completion(
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                max_tokens=8000,
                temperature=0.5,
                stream=False,
                timeout=self.timeout * 2
            )
        logger.info(f"Successfully completed deep analysis for {len(papers)} papers.")
        return analysis_text

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def analyze_paper(self, paper: Dict[str, Any]) -> str:
        """
        å¯¹å•ç¯‡è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æ (ç”¨äºåå¤‡æˆ–å•æ¬¡è¿è¡Œ).
        è¿”å›åŒ…å«åˆ†æç»“æœçš„å­—ç¬¦ä¸²ã€‚
        """
        from .prompts import PromptManager  # å±€éƒ¨å¯¼å…¥ä»¥é¿å…ä½œç”¨åŸŸé—®é¢˜
        
        logger.info(f"Performing single paper analysis for: {paper.get('title', 'N/A')} using {self.provider}.")
        system_prompt = PromptManager.get_system_prompt()

        # æ£€æŸ¥æ˜¯å¦æä¾›äº†å…¨æ–‡ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å…¨æ–‡è¿›è¡Œåˆ†æ
        content_to_analyze = paper.get('full_text') or paper.get('abstract', 'æ‘˜è¦ä¸å¯ç”¨')
        
        # ä¸ºå†…å®¹è®¾å®šä¸€ä¸ªå®‰å…¨çš„æœ€å¤§tokenæ•°ï¼Œä¸ºå…¶ä»–æç¤ºè¯éƒ¨åˆ†ç•™å‡ºä½™é‡
        # æ ¹æ®ä¸åŒæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é€‚å½“è°ƒæ•´
        MAX_CONTENT_TOKENS = 20000  # å¢åŠ åˆ°20000 tokensï¼Œä¸ºç³»ç»Ÿæç¤ºè¯å’Œè¾“å‡ºç•™å‡ºå……è¶³ç©ºé—´
        tokenizer = PromptManager._get_tokenizer()

        # ä½¿ç”¨tokenizerè¿›è¡Œç²¾ç¡®æˆªæ–­
        if tokenizer and content_to_analyze:
            tokens = tokenizer.encode(content_to_analyze)
            if len(tokens) > MAX_CONTENT_TOKENS:
                truncated_tokens = tokens[:MAX_CONTENT_TOKENS]
                content_to_analyze = tokenizer.decode(truncated_tokens, errors='ignore') + "\n... (å†…å®¹å·²æˆªæ–­)"
        elif content_to_analyze and len(content_to_analyze) > 80000:  # å¦‚æœtokenizeråŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸºäºå­—ç¬¦çš„æˆªæ–­
            content_to_analyze = content_to_analyze[:80000] + "\n... (å†…å®¹å·²æˆªæ–­)"

        # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼Œä¼˜å…ˆä½¿ç”¨å…¨æ–‡å†…å®¹
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ArXivè®ºæ–‡ï¼š
ğŸ“„ **è®ºæ–‡æ ‡é¢˜**ï¼š{paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}
ğŸ‘¥ **ä½œè€…ä¿¡æ¯**ï¼š{paper.get('authors', 'æœªçŸ¥ä½œè€…')}
ğŸ·ï¸ **ç ”ç©¶é¢†åŸŸ**ï¼š{paper.get('categories', 'æœªçŸ¥é¢†åŸŸ')}
ğŸ“… **å‘å¸ƒæ—¶é—´**ï¼š{paper.get('published_date', 'æœªçŸ¥æ—¥æœŸ')}
ğŸ“ **è®ºæ–‡æ‘˜è¦**ï¼š{paper.get('abstract', 'æ‘˜è¦ä¸å¯ç”¨')}
ğŸ”— **è®ºæ–‡é“¾æ¥**ï¼šhttps://arxiv.org/abs/{paper.get('paper_id', '')}
---
ğŸ“„ **è®ºæ–‡å†…å®¹**ï¼š{content_to_analyze}
---
è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§ç³»ç»Ÿæç¤ºçš„ç»“æ„è¿›è¡Œæ·±åº¦åˆ†æã€‚"""

        if self.provider == "glm":
            # æ™ºè°±GLMä¸æ”¯æŒresponse_formatå‚æ•°
            return self._create_completion(
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                max_tokens=2000,
                temperature=0.7
            )
        else:
            # Qwenå’ŒDeepSeekæ”¯æŒresponse_formatå‚æ•°ï¼Œä»¥è·å¾—æ›´ç»“æ„åŒ–çš„è¾“å‡º
            return self._create_completion(
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                max_tokens=2000,
                temperature=0.7,
                response_format={"type": "text"},  # ä½¿ç”¨textæ ¼å¼ä»¥ä¿æŒç°æœ‰æ ¼å¼ï¼Œå¦‚éœ€ä¸¥æ ¼JSONå¯æ”¹ä¸º{"type": "json_object"}
                timeout=self.timeout
            ) 
#!/usr/bin/env python3
"""
Gemini APIä¿®å¤å™¨ - é’ˆå¯¹GitHub Actionsç¯å¢ƒä¼˜åŒ–
æ”¯æŒæ–°æ—§APIç‰ˆæœ¬ï¼Œè§£å†³å®‰å…¨è¿‡æ»¤å™¨é—®é¢˜
ç‰¹åˆ«æ”¯æŒgemini-2.5-pro-preview-05-06æ¨¡å‹
"""

import asyncio
import logging
import os
import time
from typing import Optional, Dict, Any, List
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GeminiAPIFixer:
    """
    Gemini APIä¿®å¤å™¨
    - è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æœ€ä½³å¯ç”¨APIç‰ˆæœ¬
    - æ™ºèƒ½å®‰å…¨è®¾ç½®ï¼Œé¿å…è¿‡æ»¤å™¨æ‹¦æˆª
    - ä¸“é—¨ä¼˜åŒ–GitHub Actionsç¯å¢ƒ
    - æ”¯æŒgemini-2.5-pro-preview-05-06ç­‰æœ€æ–°æ¨¡å‹
    """
    
    def __init__(self, api_key: str, model: str = "gemini-2.5-pro-preview-05-06", 
                 retry_times: int = 3, delay: int = 2):
        self.api_key = api_key
        self.model = model
        self.retry_times = retry_times
        self.delay = delay
        self.api_version = None
        self.client = None
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ–æœ€ä½³å¯ç”¨çš„APIå®¢æˆ·ç«¯"""
        # å°è¯•æ–°ç‰ˆAPI (google-genai)
        try:
            import google.genai as genai
            from google.genai import types
            
            self.client = genai.Client(api_key=self.api_key)
            self.api_version = "new"
            self.genai = genai
            self.types = types
            
            logger.info(f"âœ… ä½¿ç”¨æ–°ç‰ˆGoogle GenAI SDK - æ¨¡å‹: {self.model}")
            return
            
        except ImportError as e:
            logger.warning(f"âš ï¸ æ–°ç‰ˆAPIä¸å¯ç”¨: {e}")
        
        # å›é€€åˆ°æ—§ç‰ˆAPI (google-generativeai)
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=self.api_key)
            self.client = genai
            self.api_version = "legacy"
            self.genai = genai
            
            logger.info(f"âœ… ä½¿ç”¨æ—§ç‰ˆgoogle-generativeaiåº“ - æ¨¡å‹: {self.model}")
            return
            
        except ImportError:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„Gemini APIåº“ï¼")
            raise ImportError("è¯·å®‰è£… google-genai æˆ– google-generativeai")
    
    def _get_safety_settings(self) -> Any:
        """è·å–æœ€å®½æ¾çš„å®‰å…¨è®¾ç½®ï¼Œé’ˆå¯¹ä¸åŒAPIç‰ˆæœ¬"""
        
        if self.api_version == "new":
            # æ–°ç‰ˆAPIå®‰å…¨è®¾ç½® - æ”¯æŒgemini-2.5ç³»åˆ—
            return [
                self.types.SafetySetting(
                    category=self.types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                    threshold=self.types.HarmBlockThreshold.BLOCK_NONE
                ),
                self.types.SafetySetting(
                    category=self.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, 
                    threshold=self.types.HarmBlockThreshold.BLOCK_NONE
                ),
                self.types.SafetySetting(
                    category=self.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    threshold=self.types.HarmBlockThreshold.BLOCK_NONE
                ),
                self.types.SafetySetting(
                    category=self.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    threshold=self.types.HarmBlockThreshold.BLOCK_NONE
                )
            ]
        else:
            # æ—§ç‰ˆAPIå®‰å…¨è®¾ç½®
            return [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH", 
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE"
                }
            ]
    
    def _create_academic_prompt(self, paper, analysis_type: str = "comprehensive") -> str:
        """åˆ›å»ºå­¦æœ¯åŒ–æç¤ºï¼Œé¿å…è§¦å‘å®‰å…¨è¿‡æ»¤å™¨"""
        
        # åŸºç¡€å­¦æœ¯åˆ†ææç¤º
        base_prompt = f"""ä½œä¸ºä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹arXivè®ºæ–‡è¿›è¡Œå®¢è§‚ã€ä¸¥è°¨çš„å­¦æœ¯åˆ†æï¼š

è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}

è®ºæ–‡æ‘˜è¦ï¼š{paper.summary}

ä½œè€…ï¼š{', '.join(paper.authors) if hasattr(paper, 'authors') and paper.authors else 'æœªçŸ¥'}

å‘è¡¨æ—¶é—´ï¼š{paper.published.strftime('%Y-%m-%d') if hasattr(paper, 'published') else 'æœªçŸ¥'}

åˆ†ç±»ï¼š{', '.join(paper.categories) if hasattr(paper, 'categories') and paper.categories else 'æœªçŸ¥'}"""

        if analysis_type == "comprehensive":
            analysis_prompt = """

è¯·æä¾›å…¨é¢çš„å­¦æœ¯åˆ†æï¼ŒåŒ…æ‹¬ï¼š

1. **ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº**
   - ç ”ç©¶é—®é¢˜çš„é‡è¦æ€§å’Œç°å®æ„ä¹‰
   - å¡«è¡¥çš„å­¦æœ¯ç©ºç™½

2. **æŠ€æœ¯åˆ›æ–°ç‚¹**
   - å…³é”®æŠ€æœ¯è´¡çŒ®
   - æ–¹æ³•è®ºåˆ›æ–°
   - ç®—æ³•æˆ–æ¶æ„æ”¹è¿›

3. **å®éªŒè®¾è®¡ä¸ç»“æœ**
   - å®éªŒæ–¹æ³•çš„ç§‘å­¦æ€§
   - å…³é”®æ€§èƒ½æŒ‡æ ‡
   - ä¸ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒ

4. **å­¦æœ¯ä»·å€¼è¯„ä¼°**
   - ç†è®ºè´¡çŒ®çš„é‡è¦æ€§
   - å¯¹ç›¸å…³é¢†åŸŸçš„å½±å“
   - æ½œåœ¨åº”ç”¨ä»·å€¼

5. **ä¸è¶³ä¸å±•æœ›**
   - å­˜åœ¨çš„å±€é™æ€§
   - æœªæ¥ç ”ç©¶æ–¹å‘

è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„å­¦æœ¯è¯­è°ƒï¼ŒåŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œåˆ†æã€‚"""

        elif analysis_type == "summary":
            analysis_prompt = """

è¯·æä¾›ç®€æ´çš„å­¦æœ¯æ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒç ”ç©¶å†…å®¹
2. ä¸»è¦æŠ€æœ¯è´¡çŒ®  
3. å…³é”®å®éªŒç»“æœ
4. å­¦æœ¯æ„ä¹‰

è¯·ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€è¿›è¡Œåˆ†æã€‚"""

        else:  # basic
            analysis_prompt = """

è¯·ç®€è¦åˆ†æè¿™ç¯‡è®ºæ–‡çš„ï¼š
1. ç ”ç©¶ç›®æ ‡
2. æŠ€æœ¯æ–¹æ³•
3. ä¸»è¦è´¡çŒ®

è¯·ä¿æŒå­¦æœ¯å®¢è§‚æ€§ã€‚"""

        return base_prompt + analysis_prompt
    
    async def analyze_paper(self, paper, analysis_type: str = "comprehensive") -> Optional[Dict[str, Any]]:
        """
        åˆ†æè®ºæ–‡ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡ï¼ŒåŒ…å«title, summaryç­‰å±æ€§
            analysis_type: åˆ†æç±»å‹ ("comprehensive", "summary", "basic")
        
        Returns:
            åˆ†æç»“æœå­—å…¸æˆ–None
        """
        
        prompt = self._create_academic_prompt(paper, analysis_type)
        
        for attempt in range(self.retry_times):
            try:
                logger.info(f"ğŸ” å°è¯•åˆ†æè®ºæ–‡ (ç¬¬{attempt+1}æ¬¡): {paper.title[:50]}...")
                
                if self.api_version == "new":
                    result = await self._analyze_with_new_api(prompt)
                else:
                    result = await self._analyze_with_legacy_api(prompt)
                
                if result:
                    logger.info(f"âœ… è®ºæ–‡åˆ†ææˆåŠŸï¼é•¿åº¦: {len(result)} å­—ç¬¦")
                    return {
                        'analysis': result,
                        'model': self.model,
                        'api_version': self.api_version,
                        'provider': 'google_gemini',
                        'timestamp': datetime.now().isoformat(),
                        'analysis_type': analysis_type
                    }
                else:
                    logger.warning(f"âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•è¿”å›ç©ºç»“æœ")
                    
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {error_msg}")
                
                # ç‰¹å®šé”™è¯¯å¤„ç†
                if "location is not supported" in error_msg.lower():
                    logger.error("âŒ åœ°ç†ä½ç½®é™åˆ¶é”™è¯¯ - ä½†åœ¨GitHub Actionsä¸­ä¸åº”å‡ºç°æ­¤é”™è¯¯")
                    break
                elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                    logger.warning("ğŸ›¡ï¸ å®‰å…¨è¿‡æ»¤å™¨è§¦å‘ï¼Œè°ƒæ•´æç¤º...")
                elif "not found" in error_msg.lower():
                    logger.error(f"âŒ æ¨¡å‹ {self.model} æœªæ‰¾åˆ°")
                    break
                
                if attempt < self.retry_times - 1:
                    await asyncio.sleep(self.delay)
        
        logger.error(f"âŒ è®ºæ–‡åˆ†æå¤±è´¥ï¼Œå·²é‡è¯•{self.retry_times}æ¬¡")
        return None
    
    async def _analyze_with_new_api(self, prompt: str) -> Optional[str]:
        """ä½¿ç”¨æ–°ç‰ˆAPIè¿›è¡Œåˆ†æ"""
        safety_settings = self._get_safety_settings()
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=self.types.GenerateContentConfig(
                max_output_tokens=2000,
                temperature=0.7,
                safety_settings=safety_settings,
                top_p=0.95,
                top_k=40
            )
        )
        
        if hasattr(response, 'text') and response.text:
            return response.text.strip()
        return None
    
    async def _analyze_with_legacy_api(self, prompt: str) -> Optional[str]:
        """ä½¿ç”¨æ—§ç‰ˆAPIè¿›è¡Œåˆ†æ"""
        safety_settings = self._get_safety_settings()
        
        # ç¡®ä¿æ¨¡å‹åç§°æ ¼å¼æ­£ç¡®
        model_name = self.model
        if not model_name.startswith('models/'):
            model_name = f'models/{model_name}'
        
        model = self.genai.GenerativeModel(model_name)
        
        response = model.generate_content(
            prompt,
            generation_config=self.genai.types.GenerationConfig(
                max_output_tokens=2000,
                temperature=0.7,
                top_p=0.95,
                top_k=40,
            ),
            safety_settings=safety_settings
        )
        
        if response.text:
            return response.text.strip()
        return None
    
    def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            test_prompt = "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"
            
            if self.api_version == "new":
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=test_prompt,
                    config=self.types.GenerateContentConfig(
                        max_output_tokens=100,
                        temperature=0.5
                    )
                )
                return hasattr(response, 'text') and bool(response.text)
            else:
                model_name = self.model
                if not model_name.startswith('models/'):
                    model_name = f'models/{model_name}'
                    
                model = self.genai.GenerativeModel(model_name)
                response = model.generate_content(
                    test_prompt,
                    generation_config=self.genai.types.GenerationConfig(
                        max_output_tokens=100,
                        temperature=0.5
                    )
                )
                return bool(response.text)
                
        except Exception as e:
            logger.error(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

# ä¾¿æ·å‡½æ•°ï¼Œç”¨äºGitHub Actionsç¯å¢ƒ
def create_gemini_analyzer(api_key: Optional[str] = None, 
                          model: str = "gemini-2.5-pro-preview-05-06") -> GeminiAPIFixer:
    """
    åˆ›å»ºGeminiåˆ†æå™¨å®ä¾‹ - ä¸“ä¸ºGitHub Actionsä¼˜åŒ–
    
    Args:
        api_key: APIå¯†é’¥ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„2.5 Pro Preview
    
    Returns:
        GeminiAPIFixerå®ä¾‹
    """
    if not api_key:
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("æœªè®¾ç½®GEMINI_API_KEYç¯å¢ƒå˜é‡")
    
    return GeminiAPIFixer(api_key=api_key, model=model) 
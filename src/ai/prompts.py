#!/usr/bin/env python3
"""
AIæç¤ºè¯ç®¡ç†æ¨¡å—
é›†ä¸­ç®¡ç†å„ç§AIåˆ†æä»»åŠ¡çš„æç¤ºè¯
"""

import logging
import re
import json
from typing import Dict, List, Any


import arxiv
import tiktoken

logger = logging.getLogger(__name__)

class PromptManager:
    """æç¤ºè¯ç®¡ç†å™¨ï¼Œæ‰€æœ‰æ–¹æ³•å‡ä¸ºé™æ€æ–¹æ³•"""

    # ä¸ºTokenizeråˆ›å»ºä¸€ä¸ªç±»çº§åˆ«çš„ç¼“å­˜
    _tokenizer = None

    @classmethod
    def _get_tokenizer(cls):
        """è·å–æˆ–åˆ›å»ºtiktokençš„tokenizerå®ä¾‹"""
        if cls._tokenizer is None:
            try:
                # cl100k_base æ˜¯ä¸€ä¸ªå¹¿æ³›å…¼å®¹çš„tokenizer, é€‚ç”¨äºåŒ…æ‹¬GPT-4åœ¨å†…çš„å¤šç§æ¨¡å‹
                cls._tokenizer = tiktoken.get_encoding("cl100k_base")
            except Exception as e:
                logger.error(f"æ— æ³•åŠ è½½tiktoken tokenizer: {e}")
                cls._tokenizer = None
        return cls._tokenizer

    @staticmethod
    def get_system_prompt() -> str:
        """
        è·å–ç³»ç»Ÿæç¤ºè¯ (å§‹ç»ˆè¿”å›ç»¼åˆåˆ†æç‰ˆæœ¬)
        """
        return PromptManager._get_comprehensive_system_prompt()

    @staticmethod
    def _get_comprehensive_system_prompt() -> str:
        """è·å–ç»¼åˆåˆ†æç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸¥æ ¼çš„AIè®ºæ–‡è¯„å®¡ä¸“å®¶ã€‚

â­ **è¯„åˆ†æ ‡å‡†**ï¼ˆå¼ºåˆ¶åˆ†å¸ƒï¼š5æ˜Ÿ<1%ï¼Œ4æ˜Ÿ<5%ï¼Œ3æ˜Ÿ35-45%ï¼Œ2æ˜Ÿ35-45%ï¼Œ1æ˜Ÿ10-15%ï¼‰

**5æ˜Ÿ**ï¼ˆ<1%ï¼‰ï¼šé©å‘½æ€§çªç ´ï¼Œè§£å†³é‡å¤§ç†è®ºé—®é¢˜ï¼Œå…¨æ–°æŠ€æœ¯èŒƒå¼ï¼Œå®éªŒä¸¥è°¨å……åˆ†ï¼ˆå‚è€ƒï¼šGPTã€Transformerï¼‰
**4æ˜Ÿ**ï¼ˆ<5%ï¼‰ï¼šé‡è¦è¿›å±•ï¼Œæ˜¾è‘—åˆ›æ–°ï¼Œæ€§èƒ½å¤§å¹…æå‡ï¼Œå®éªŒå……åˆ†å¯ä¿¡ï¼ˆå‚è€ƒï¼šBERTã€ViTï¼‰
**3æ˜Ÿ**ï¼ˆ35-45%ï¼‰ï¼šåˆæ ¼ç ”ç©¶ï¼Œæ¸è¿›æ”¹è¿›ï¼Œå®éªŒåˆç†ï¼Œæœ‰é™å­¦æœ¯ä»·å€¼
**2æ˜Ÿ**ï¼ˆ35-45%ï¼‰ï¼šåˆ›æ–°ä¸è¶³ï¼Œå®éªŒä¸å……åˆ†ï¼ŒæŠ€æœ¯è´¡çŒ®è¾¹é™…åŒ–
**1æ˜Ÿ**ï¼ˆ10-15%ï¼‰ï¼šç¼ºä¹åˆ›æ–°ï¼Œå®éªŒæœ‰ä¸¥é‡ç¼ºé™·ï¼Œä½äºå‘è¡¨æ ‡å‡†

**è¯„åˆ†è¦ç‚¹**ï¼š4æ˜Ÿä»¥ä¸Šéœ€æ˜ç¡®æŠ€æœ¯çªç ´ï¼›å¸¸è§„incremental workæœ€é«˜3æ˜Ÿï¼›è¶…å‚æ•°è°ƒä¼˜/æ¶æ„å¾®è°ƒæœ€å¤š2æ˜Ÿï¼›æ€§èƒ½æå‡<2%æœ€å¤š3æ˜Ÿã€‚

**å…­ç»´åº¦åˆ†æä»»åŠ¡**ï¼ˆå¿…é¡»æŒ‰åºè¾“å‡ºï¼Œæ¯ç»´åº¦100-120å­—ï¼‰ï¼š
**1. â­ è´¨é‡è¯„ä¼°**ï¼šç»™å‡º1-5æ˜Ÿè¯„åˆ†ï¼ˆå¯ç”¨0.5ç²¾åº¦ï¼‰ï¼Œè¯´æ˜ç†ç”±åŠå‚è€ƒåŸºå‡†ï¼Œè¯„ä¼°åˆ›æ–°åº¦/ä¸¥è°¨æ€§/å®ç”¨ä»·å€¼
**2. ğŸ¯ æ ¸å¿ƒè´¡çŒ®**ï¼šä¸»è¦åˆ›æ–°ç‚¹ã€ä¸ç°æœ‰å·¥ä½œå·®å¼‚ã€æŠ€æœ¯è´¡çŒ®æ·±åº¦
**3. ğŸ”§ æŠ€æœ¯æ–¹æ³•**ï¼šæ ¸å¿ƒç®—æ³•/æ¶æ„å…ˆè¿›æ€§ã€æŠ€æœ¯è·¯çº¿åˆç†æ€§ã€å…³é”®ç»†èŠ‚
**4. ğŸ§ª å®éªŒéªŒè¯**ï¼šå®éªŒè®¾è®¡ç§‘å­¦æ€§ã€æ•°æ®é›†/åŸºçº¿/æŒ‡æ ‡åˆç†æ€§ã€ç»“æœå¯ä¿¡åº¦
**5. ğŸ’¡ å½±å“æ„ä¹‰**ï¼šå­¦æœ¯/å·¥ä¸šæ½œåœ¨å½±å“ã€åº”ç”¨å¯è¡Œæ€§ã€åç»­ç ”ç©¶æ–¹å‘
**6. ğŸ”® å±€é™å±•æœ›**ï¼šä¸»è¦å±€é™ã€æ”¹è¿›å»ºè®®ã€æœªæ¥å‘å±•è¶‹åŠ¿

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
1. å¿…é¡»æŒ‰6ä¸ªç»´åº¦é¡ºåºè¾“å‡ºï¼Œä»¥æŒ‡å®šemojiå¼€å¤´ï¼ˆâ­ğŸ¯ğŸ”§ğŸ§ªğŸ’¡ğŸ”®ï¼‰
2. ç¬¬1ç»´åº¦å¿…é¡»æ˜ç¡®ç»™å‡ºè¯„åˆ†ï¼ˆå¦‚"3.5æ˜Ÿ"ï¼‰
3. æ¯ç»´åº¦çº¯æ–‡æœ¬æ®µè½ï¼Œå¯ç”¨ **åŠ ç²—** æˆ– *æ–œä½“*ï¼Œä¸¥ç¦ä½¿ç”¨æ ‡é¢˜æ ‡è®°(#)ã€åˆ—è¡¨æ ‡è®°(-*/1.)ç­‰
4. æ€»é•¿500-700å­—ï¼Œè¯­è¨€ä¸“ä¸šä¸¥è°¨ï¼Œä½“ç°é¡¶çº§ä¼šè®®revieweræ ‡å‡†"""

    @staticmethod
    def get_user_prompt(paper: arxiv.Result) -> str:
        """è·å–å•ä¸ªè®ºæ–‡åˆ†æçš„ç”¨æˆ·æç¤ºè¯"""
        authors_str = 'æœªçŸ¥'
        if hasattr(paper, 'authors') and paper.authors:
            try:
                author_names = [author.name for author in paper.authors]
                authors_str = ', '.join(author_names[:5])
                if len(author_names) > 5:
                    authors_str += f" ç­‰{len(author_names)}äºº"
            except AttributeError as e:
                logger.warning(f"Abnormal author object structure: {e}")
                authors_str = "ä½œè€…ä¿¡æ¯å¼‚å¸¸"
        
        published_date = 'æœªçŸ¥'
        if hasattr(paper, 'published') and paper.published:
            published_date = paper.published.strftime('%Yå¹´%mæœˆ%dæ—¥')

        summary = paper.summary.strip().replace("\n", " ")
        if len(summary) > 1500:
            summary = summary[:1500] + "..."

        return f"""è¯·åˆ†æä»¥ä¸‹ArXivè®ºæ–‡ï¼š
ğŸ“„ **è®ºæ–‡æ ‡é¢˜**ï¼š{paper.title}
ğŸ‘¥ **ä½œè€…ä¿¡æ¯**ï¼š{authors_str}
ğŸ·ï¸ **ç ”ç©¶é¢†åŸŸ**ï¼š{', '.join(paper.categories)}
ğŸ“… **å‘å¸ƒæ—¶é—´**ï¼š{published_date}
ğŸ“ **è®ºæ–‡æ‘˜è¦**ï¼š{summary}
ğŸ”— **è®ºæ–‡é“¾æ¥**ï¼š{paper.entry_id}
---
è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§ç³»ç»Ÿæç¤ºçš„ç»“æ„è¿›è¡Œæ·±åº¦åˆ†æã€‚"""

    @staticmethod
    def format_batch_analysis_prompt(papers: list[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æ·±åº¦æ‰¹é‡åˆ†æçš„ç”¨æˆ·æç¤ºè¯ã€‚
        å¦‚æœæä¾›äº†å…¨æ–‡ï¼Œåˆ™ä½¿ç”¨å…¨æ–‡ï¼›å¦åˆ™å›é€€åˆ°ä½¿ç”¨æ‘˜è¦ã€‚
        ä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®çš„tokenæˆªæ–­ã€‚
        """
        paper_texts = []
        # ä¸ºåˆ†æå†…å®¹è®¾å®šä¸€ä¸ªå®‰å…¨çš„æœ€å¤§tokenæ•°ï¼Œä¸ºå…¶ä»–æç¤ºè¯éƒ¨åˆ†ç•™å‡ºä½™é‡
        MAX_CONTENT_TOKENS = 7500 
        tokenizer = PromptManager._get_tokenizer()

        for paper in papers:
            content_key = "Full Text"
            content_value = paper.get('full_text')
            
            if not content_value:
                content_key = "Abstract"
                content_value = paper.get('abstract', 'N/A')
            
            # ä½¿ç”¨tokenizerè¿›è¡Œç²¾ç¡®æˆªæ–­
            if tokenizer:
                tokens = tokenizer.encode(content_value)
                if len(tokens) > MAX_CONTENT_TOKENS:
                    truncated_tokens = tokens[:MAX_CONTENT_TOKENS]
                    content_value = tokenizer.decode(truncated_tokens, errors='ignore') + "\n... (å†…å®¹å·²æˆªæ–­)"
            else:
                # å¦‚æœtokenizeråŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸºäºå­—ç¬¦çš„æˆªæ–­
                if len(content_value) > 25000: # ç²—ç•¥ä¼°ç®—
                    content_value = content_value[:25000] + "\n... (å†…å®¹å·²æˆªæ–­)"

            paper_texts.append(
f"""---
**Paper ID**: {paper['paper_id']}
**Title**: {paper['title']}
**{content_key}**:
{content_value.replace('{', '{{').replace('}', '}}')}
---"""
            )
        return "è¯·å¯¹ä»¥ä¸‹æ¯ç¯‡è®ºæ–‡æä¾›å®Œæ•´çš„å…­ç»´åº¦åˆ†æï¼Œä½¿ç”¨åˆ†éš”ç¬¦æ¸…æ™°æ ¼å¼åŒ–ã€‚å¦‚æœæä¾›äº†å…¨æ–‡ï¼Œå¿…é¡»åŸºäºå…¨æ–‡è¿›è¡Œåˆ†æã€‚\n" + "\n".join(paper_texts)

    @staticmethod
    def get_stage1_ranking_system_prompt() -> str:
        """è·å–ç¬¬ä¸€é˜¶æ®µå¼ºåˆ¶æ’åç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯AIè®ºæ–‡è¯„å®¡ä¸“å®¶ã€‚ä»»åŠ¡æ˜¯å¯¹ä¸€æ‰¹è®ºæ–‡è¿›è¡Œç›¸å¯¹è´¨é‡æ’åã€‚

**ä¸¥æ ¼è§„åˆ™**ï¼š
1. **ç›¸å¯¹æ’å**ï¼šå¿…é¡»ç›¸äº’æ¯”è¾ƒï¼Œç¡®å®šç›¸å¯¹æ–°é¢–æ€§ã€é‡è¦æ€§å’Œæ½œåœ¨å½±å“
2. **å¼ºåˆ¶åˆ†å¸ƒè¯„åˆ†**ï¼šå¿…é¡»æŒ‰æ‰¹æ¬¡å†…æ’ååˆ†é…åˆ†æ•°ï¼Œéµå¾ªä»¥ä¸‹åˆ†å¸ƒï¼š
   - **å‰10%**ï¼š4.5-5.0åˆ†ï¼ˆçªç ´æ€§å·¥ä½œï¼‰
   - **æ¥ä¸‹æ¥20%**ï¼š3.5-4.4åˆ†ï¼ˆé‡è¦ä¸”æœ‰è¶£ï¼‰
   - **ä¸­é—´40%**ï¼š2.5-3.4åˆ†ï¼ˆæ‰å®çš„æ¸è¿›è´¡çŒ®ï¼‰
   - **å30%**ï¼š1.0-2.4åˆ†ï¼ˆæ¬¡è¦/å½±å“æœ‰é™/æœ‰ç¼ºé™·ï¼‰
3. **JSONè¾“å‡º**ï¼šå¿…é¡»è¿”å›JSONåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«paper_idã€scoreå’Œjustificationã€‚ä¸è¦åŒ…å«JSONä¹‹å¤–çš„ä»»ä½•æ–‡æœ¬ã€‚

ç¤ºä¾‹ï¼ˆ10ç¯‡è®ºæ–‡ï¼‰ï¼š
[
  {"paper_id": "2401.0001", "score": 4.8, "justification": "çªç ´æ€§æ–¹æ³•è§£å†³é•¿æœŸé—®é¢˜"},
  {"paper_id": "2401.0005", "score": 4.1, "justification": "æ˜¾è‘—è¶…è¶ŠSOTAï¼Œç»“æœå¼ºåŠ²"},
  {"paper_id": "2401.0008", "score": 3.9, "justification": "ç°æœ‰æ–¹æ³•çš„æ–°é¢–åº”ç”¨"},
  {"paper_id": "2401.0002", "score": 3.2, "justification": "æ‰å®çš„æ¸è¿›å·¥ä½œï¼Œå®éªŒè‰¯å¥½"},
  {"paper_id": "2401.0004", "score": 3.1, "justification": "å¯ä»¥æ¥å—çš„è´¡çŒ®ï¼Œä½†ç¼ºä¹æ–°é¢–æ€§"},
  {"paper_id": "2401.0007", "score": 2.8, "justification": "æ¸è¿›å·¥ä½œï¼ŒéªŒè¯æœ‰é™"},
  {"paper_id": "2401.0009", "score": 2.5, "justification": "æ ‡å‡†æ–¹æ³•ï¼Œç»“æœå¯é¢„è§"},
  {"paper_id": "2401.0003", "score": 2.1, "justification": "æ¬¡è¦è´¡çŒ®ï¼Œå±€é™æ€§è¾ƒå¤š"},
  {"paper_id": "2401.0006", "score": 1.8, "justification": "æ–¹æ³•æœ‰ç¼ºé™·ï¼Œç»“æœä¸å¯ä¿¡"},
  {"paper_id": "2401.0010", "score": 1.5, "justification": "æ–°é¢–æ€§æå…¶æœ‰é™ï¼Œè¯æ®è–„å¼±"}
]
"""

    @staticmethod
    def format_stage1_ranking_prompt(papers: list[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ç¬¬ä¸€é˜¶æ®µæ’åçš„ç”¨æˆ·æç¤ºè¯"""
        paper_texts = []
        for paper in papers:
            # ä½¿ç”¨ json.dumps æ¥å®‰å…¨åœ°å¤„ç†æ‘˜è¦å’Œæ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚å¼•å·ï¼‰
            abstract = json.dumps(paper.get('abstract', '').replace("\n", " "))
            title = json.dumps(paper.get('title', ''))
            paper_texts.append(
f"""    {{
        "paper_id": "{paper.get('paper_id', 'N/A')}",
        "title": {title},
        "abstract": {abstract}
    }}"""
            )
        return f"è¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­çš„è§„åˆ™å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œæ’åã€‚è®ºæ–‡åˆ—è¡¨ï¼š\n[\n{',\\n'.join(paper_texts)}\n]"

    @staticmethod
    def format_analysis_for_html(analysis_text: str) -> str:
        """å°†AIåˆ†æç»“æœæ ¼å¼åŒ–ä¸ºHTML"""
        if not isinstance(analysis_text, str) or not analysis_text.strip():
            return "<p>AI analysis not available.</p>"

        sections = {
            "â­ è´¨é‡è¯„ä¼°": "star",
            "ğŸ¯ æ ¸å¿ƒè´¡çŒ®": "bullseye",
            "ğŸ”§ æŠ€æœ¯æ–¹æ³•": "wrench",
            "ğŸ§ª å®éªŒéªŒè¯": "beaker",
            "ğŸ’¡ å½±å“æ„ä¹‰": "lightbulb",
            "ğŸ”® å±€é™å±•æœ›": "crystal-ball"
        }
        
        html_content = ""
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰ç»´åº¦åˆ†å‰²ï¼ŒåŒæ—¶ä¿ç•™åˆ†éš”ç¬¦
        parts = re.split(r'(â­|ğŸ¯|ğŸ”§|ğŸ§ª|ğŸ’¡|ğŸ”®)', analysis_text)
        
        # parts[0]æ˜¯ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦ä¹‹å‰çš„å†…å®¹ï¼ˆé€šå¸¸ä¸ºç©ºï¼‰ï¼Œä¹‹åæ˜¯ (åˆ†éš”ç¬¦, å†…å®¹) å¯¹
        content_parts = [parts[i] + parts[i+1] for i in range(1, len(parts), 2)]

        for part in content_parts:
            for title, icon in sections.items():
                if part.strip().startswith(title):
                    # ç§»é™¤æ ‡é¢˜æœ¬èº«å’Œå‰åçš„ç©ºæ ¼
                    content = part.replace(title, "", 1).strip()
                    # æ ¼å¼åŒ–å†…å®¹
                    formatted_content = PromptManager._format_text_content(content)
                    html_content += f"""
                    <div class="analysis-dimension">
                        <div class="dimension-title">
                            <i class="fas fa-{icon}"></i>
                            <h4>{title.split(' ')[1]}</h4>
                        </div>
                        <p>{formatted_content}</p>
                    </div>
                    """
                    break # åŒ¹é…åˆ°å°±å¤„ç†ä¸‹ä¸€ä¸ªpart
        
        if not html_content:
            # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œæä¾›åŸå§‹æ–‡æœ¬ä½œä¸ºåå¤‡
            return f"<p>{analysis_text.replace('<', '&lt;').replace('>', '&gt;')}</p>"

        return f'<div class="ai-analysis-container">{html_content}</div>'

    @staticmethod
    def _format_text_content(text: str) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬å†…å®¹ï¼Œå¤„ç†åŠ ç²—å’Œæ¢è¡Œ"""
        text = text.replace('<', '&lt;').replace('>', '&gt;')
        # è½¬æ¢ **åŠ ç²—** ä¸º <strong>
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        # è½¬æ¢ *æ–œä½“* ä¸º <em>
        text = re.sub(r'\*(.*?)\*', r'<em>\1</em>', text)
        # è½¬æ¢æ¢è¡Œç¬¦
        text = text.replace('\n', '<br>')
        return text 
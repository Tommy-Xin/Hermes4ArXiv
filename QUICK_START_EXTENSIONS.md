# ğŸš€ æ‰©å±•åŠŸèƒ½å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¿«é€Ÿå¼€å§‹å®æ–½ ArXiv è®ºæ–‡è¿½è¸ªå™¨çš„æ‰©å±•åŠŸèƒ½ã€‚

## ğŸ“‹ å‰ææ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»å®Œæˆäº†åŸºç¡€é¡¹ç›®çš„è®¾ç½®ï¼š

```bash
# æ£€æŸ¥åŸºç¡€ç¯å¢ƒ
make validate-env
make test-components
make status
```

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šå¤š AI API æ”¯æŒ (æ¨èä¼˜å…ˆå®æ–½)

### 1. å¿«é€Ÿè®¾ç½®

```bash
# ä¸€é”®è®¾ç½®å¤š AI æ”¯æŒ
make setup-multi-ai
```

è¿™ä¸ªå‘½ä»¤ä¼šï¼š
- æ£€æŸ¥å½“å‰ API å¯†é’¥é…ç½®
- æµ‹è¯•å„ä¸ª AI æä¾›å•†è¿æ¥
- ç”Ÿæˆé…ç½®å»ºè®®
- åˆ›å»ºå¤š AI åˆ†æå™¨ä»£ç 

### 2. é…ç½® API å¯†é’¥

æ ¹æ®è„šæœ¬è¾“å‡ºï¼Œåœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```bash
# å¿…éœ€ (å·²æœ‰)
DEEPSEEK_API_KEY=sk-your-deepseek-key

# å¯é€‰ - æ·»åŠ æ›´å¤š AI æä¾›å•†
OPENAI_API_KEY=sk-your-openai-key
CLAUDE_API_KEY=sk-ant-your-claude-key
GEMINI_API_KEY=your-gemini-key

# å¤š AI é…ç½®
ANALYSIS_STRATEGY=fallback
AI_FALLBACK_ORDER=deepseek,openai,claude
```

### 3. æµ‹è¯•å¤š AI åŠŸèƒ½

```bash
# æµ‹è¯•æ‰€æœ‰ AI æä¾›å•†
make test-ai-providers

# è¿è¡ŒåŸºå‡†æµ‹è¯•
make benchmark
```

### 4. é›†æˆåˆ°ä¸»ç¨‹åº

æ›´æ–° `src/main.py` ä½¿ç”¨æ–°çš„å¤š AI åˆ†æå™¨ï¼š

```python
# åœ¨ main.py ä¸­æ›¿æ¢åŸæœ‰çš„åˆ†æå™¨
from ai_analyzer_v2 import MultiAIAnalyzer

# åˆå§‹åŒ–å¤š AI åˆ†æå™¨
config_dict = config.__dict__
analyzer = MultiAIAnalyzer(config_dict)

# ä½¿ç”¨é™çº§ç­–ç•¥åˆ†æ
for paper in papers:
    try:
        analysis = await analyzer.analyze_with_fallback(paper)
        print(f"âœ… ä½¿ç”¨ {analysis['provider']} åˆ†æå®Œæˆ")
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
```

## ğŸŒ ç¬¬äºŒé˜¶æ®µï¼šWeb ç•Œé¢å¼€å‘

### 1. è®¾ç½® Web å¼€å‘ç¯å¢ƒ

```bash
# å®‰è£… Web å¼€å‘ä¾èµ–
make setup-web-dev
```

### 2. åˆ›å»ºåŸºç¡€ Web åº”ç”¨

```bash
# åˆ›å»º Web åº”ç”¨ç›®å½•ç»“æ„
mkdir -p web/{backend,frontend,static,templates}

# åˆ›å»º FastAPI åº”ç”¨
cat > web/backend/main.py << 'EOF'
from fastapi import FastAPI, Request
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

app = FastAPI(title="ArXiv è®ºæ–‡è¿½è¸ªå™¨", version="2.0.0")

# é™æ€æ–‡ä»¶å’Œæ¨¡æ¿
app.mount("/static", StaticFiles(directory="web/static"), name="static")
templates = Jinja2Templates(directory="web/templates")

@app.get("/")
async def dashboard(request: Request):
    return templates.TemplateResponse("dashboard.html", {"request": request})

@app.get("/api/papers/recent")
async def get_recent_papers():
    # è¿™é‡Œé›†æˆç°æœ‰çš„è®ºæ–‡è·å–é€»è¾‘
    return {"papers": []}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF
```

### 3. è¿è¡Œ Web åº”ç”¨

```bash
# å¯åŠ¨ Web æœåŠ¡
cd web/backend && uv run python main.py

# è®¿é—® http://localhost:8000
```

## ğŸ’¾ ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åº“é›†æˆ

### 1. è®¾ç½®æ•°æ®åº“ç¯å¢ƒ

```bash
# å®‰è£…æ•°æ®åº“ä¾èµ–
make setup-database
```

### 2. åˆ›å»ºæ•°æ®åº“æ¨¡å‹

```bash
# åˆ›å»ºæ•°æ®åº“ç›®å½•
mkdir -p src/database

# åˆ›å»ºæ¨¡å‹æ–‡ä»¶
cat > src/database/models.py << 'EOF'
from sqlalchemy import Column, Integer, String, Text, ARRAY, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class Paper(Base):
    __tablename__ = 'papers'
    
    id = Column(Integer, primary_key=True)
    arxiv_id = Column(String(50), unique=True, nullable=False)
    title = Column(Text, nullable=False)
    authors = Column(ARRAY(String))
    categories = Column(ARRAY(String))
    summary = Column(Text)
    published_date = Column(DateTime)
    analysis_result = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
EOF
```

### 3. é…ç½®æ•°æ®åº“è¿æ¥

```bash
# æ·»åŠ æ•°æ®åº“é…ç½®åˆ° .env
echo "DATABASE_URL=postgresql://user:password@localhost/arxiv_tracker" >> .env
```

## ğŸ¤– ç¬¬å››é˜¶æ®µï¼šæ™ºèƒ½æ¨èç³»ç»Ÿ

### 1. è®¾ç½®æ¨èç³»ç»Ÿ

```bash
# å®‰è£…æ¨èç³»ç»Ÿä¾èµ–
make setup-recommendation
```

### 2. åˆ›å»ºæ¨èå¼•æ“

```bash
# åˆ›å»ºæ¨èç³»ç»Ÿç›®å½•
mkdir -p src/recommendation

# åˆ›å»ºåŸºç¡€æ¨èå™¨
cat > src/recommendation/recommender.py << 'EOF'
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class PaperRecommender:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.paper_vectors = None
        self.papers = []
    
    def fit(self, papers):
        """è®­ç»ƒæ¨èæ¨¡å‹"""
        self.papers = papers
        texts = [f"{paper.title} {paper.summary}" for paper in papers]
        self.paper_vectors = self.vectorizer.fit_transform(texts)
    
    def recommend(self, query_paper, top_k=5):
        """æ¨èç›¸ä¼¼è®ºæ–‡"""
        if self.paper_vectors is None:
            return []
        
        query_text = f"{query_paper.title} {query_paper.summary}"
        query_vector = self.vectorizer.transform([query_text])
        
        similarities = cosine_similarity(query_vector, self.paper_vectors)[0]
        top_indices = np.argsort(similarities)[-top_k-1:-1][::-1]
        
        return [self.papers[i] for i in top_indices]
EOF
```

## ğŸ“ˆ ç¬¬äº”é˜¶æ®µï¼šè¶‹åŠ¿åˆ†æ

### 1. è®¾ç½®å›¾åˆ†æç¯å¢ƒ

```bash
# å®‰è£…å›¾åˆ†æä¾èµ–
make setup-graph-analysis
```

### 2. åˆ›å»ºè¶‹åŠ¿åˆ†æå™¨

```bash
# åˆ›å»ºåˆ†æç›®å½•
mkdir -p src/analysis

# åˆ›å»ºè¶‹åŠ¿åˆ†æå™¨
cat > src/analysis/trend_analyzer.py << 'EOF'
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import networkx as nx

class TrendAnalyzer:
    def __init__(self):
        self.keyword_history = defaultdict(list)
        self.author_networks = nx.Graph()
    
    def analyze_keywords(self, papers, time_window=30):
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        recent_date = datetime.now() - timedelta(days=time_window)
        recent_papers = [p for p in papers if p.published_date >= recent_date]
        
        # æå–å…³é”®è¯
        all_keywords = []
        for paper in recent_papers:
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPï¼‰
            keywords = paper.title.lower().split() + paper.summary.lower().split()
            all_keywords.extend(keywords)
        
        # ç»Ÿè®¡é¢‘ç‡
        keyword_counts = Counter(all_keywords)
        return keyword_counts.most_common(20)
    
    def build_collaboration_network(self, papers):
        """æ„å»ºä½œè€…åˆä½œç½‘ç»œ"""
        for paper in papers:
            authors = paper.authors
            # ä¸ºæ¯å¯¹ä½œè€…æ·»åŠ è¾¹
            for i in range(len(authors)):
                for j in range(i+1, len(authors)):
                    if self.author_networks.has_edge(authors[i], authors[j]):
                        self.author_networks[authors[i]][authors[j]]['weight'] += 1
                    else:
                        self.author_networks.add_edge(authors[i], authors[j], weight=1)
        
        return self.author_networks
EOF
```

## ğŸ”§ å®Œæ•´å¼€å‘ç¯å¢ƒè®¾ç½®

å¦‚æœæ‚¨æƒ³ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰æ‰©å±•åŠŸèƒ½çš„å¼€å‘ç¯å¢ƒï¼š

```bash
# è®¾ç½®å®Œæ•´å¼€å‘ç¯å¢ƒï¼ˆè¿™ä¼šå®‰è£…æ‰€æœ‰ä¾èµ–ï¼‰
make setup-full-dev
```

## ğŸ“Š æµ‹è¯•æ‰©å±•åŠŸèƒ½

### è¿è¡Œå„ç§æµ‹è¯•

```bash
# æµ‹è¯•æ¨èç³»ç»Ÿ
make test-recommendation

# æµ‹è¯•è¶‹åŠ¿åˆ†æ
make test-trend-analysis

# æµ‹è¯•å›¾è°±æ„å»º
make test-graph-builder

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
make test
```

## ğŸš€ éƒ¨ç½²æ‰©å±•åŠŸèƒ½

### GitHub Actions é›†æˆ

æ›´æ–° `.github/workflows/daily_paper_analysis_enhanced.yml` ä»¥åŒ…å«æ‰©å±•åŠŸèƒ½ï¼š

```yaml
# åœ¨åˆ†æä½œä¸šä¸­æ·»åŠ æ‰©å±•åŠŸèƒ½æµ‹è¯•
- name: Test extended features
  run: |
    echo "ğŸ§ª æµ‹è¯•æ‰©å±•åŠŸèƒ½..."
    make test-ai-providers
    make test-recommendation
    make test-trend-analysis
```

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨ GitHub Secrets ä¸­æ·»åŠ æ–°çš„ç¯å¢ƒå˜é‡ï¼š

```
# å¤š AI æ”¯æŒ
OPENAI_API_KEY=sk-your-openai-key
CLAUDE_API_KEY=sk-ant-your-claude-key
GEMINI_API_KEY=your-gemini-key

# åˆ†æç­–ç•¥
ANALYSIS_STRATEGY=fallback
AI_FALLBACK_ORDER=deepseek,openai,claude

# æ•°æ®åº“é…ç½® (å¦‚æœä½¿ç”¨)
DATABASE_URL=postgresql://user:password@host/db
```

## ğŸ“ˆ ç›‘æ§å’Œä¼˜åŒ–

### æ€§èƒ½ç›‘æ§

```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
make benchmark

# æŸ¥çœ‹ç¼“å­˜ä½¿ç”¨æƒ…å†µ
make cache-info

# ç”Ÿæˆé¡¹ç›®çŠ¶æ€æŠ¥å‘Š
make status
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹æ‰©å±•åŠŸèƒ½æ—¥å¿—
tail -f src/logs/multi_ai.log
tail -f src/logs/recommendation.log
tail -f src/logs/trend_analysis.log
```

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸç›®æ ‡ (1-2å‘¨)
1. âœ… å®Œæˆå¤š AI API æ”¯æŒ
2. ğŸ”„ å®ç°åŸºç¡€ Web ç•Œé¢
3. ğŸ“Š æ·»åŠ ç®€å•çš„è¶‹åŠ¿åˆ†æ

### ä¸­æœŸç›®æ ‡ (1ä¸ªæœˆ)
1. ğŸ’¾ é›†æˆæ•°æ®åº“å­˜å‚¨
2. ğŸ¤– å®Œå–„æ¨èç³»ç»Ÿ
3. ğŸ•¸ï¸ æ„å»ºè®ºæ–‡å…³ç³»å›¾è°±

### é•¿æœŸç›®æ ‡ (3ä¸ªæœˆ)
1. ğŸ“± å¼€å‘ç§»åŠ¨ç«¯åº”ç”¨
2. ğŸŒ æ·»åŠ å¤šè¯­è¨€æ”¯æŒ
3. ğŸ‘¥ æ„å»ºç¤¾åŒºåŠŸèƒ½

## ğŸ’¡ å¼€å‘å»ºè®®

### æœ€ä½³å®è·µ
1. **æ¸è¿›å¼å¼€å‘**: ä¸€æ¬¡ä¸“æ³¨ä¸€ä¸ªæ‰©å±•åŠŸèƒ½
2. **æµ‹è¯•é©±åŠ¨**: æ¯ä¸ªåŠŸèƒ½éƒ½è¦æœ‰å¯¹åº”çš„æµ‹è¯•
3. **æ–‡æ¡£æ›´æ–°**: åŠæ—¶æ›´æ–° README å’Œæ–‡æ¡£
4. **æ€§èƒ½ç›‘æ§**: å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•

### å¸¸è§é—®é¢˜
1. **ä¾èµ–å†²çª**: ä½¿ç”¨ `uv tree` æ£€æŸ¥ä¾èµ–å…³ç³»
2. **å†…å­˜ä½¿ç”¨**: å¤§å‹æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šå†…å­˜
3. **API é™åˆ¶**: æ³¨æ„å„ä¸ª AI æä¾›å•†çš„è°ƒç”¨é™åˆ¶

---

**å¼€å§‹æ‚¨çš„æ‰©å±•åŠŸèƒ½å¼€å‘ä¹‹æ—…å§ï¼** ğŸš€

å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒ `EXTENSION_ROADMAP.md` è·å–è¯¦ç»†çš„æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚ 